{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "cf476815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('./hf_transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cecd5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:  14:09:39\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from typing import DefaultDict, List\n",
    "\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import set_seed\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from data_reader import DataPoint, GetDataAsPython\n",
    "from prepare_data import create_data\n",
    "from prepare_data import create_dataset\n",
    "from prepare_data import extract_warning_types\n",
    "from prepare_data import filter_rule\n",
    "from utils import boolean_string\n",
    "from utils import get_scores_weighted_average\n",
    "from utils import get_current_time\n",
    "\n",
    "# transformers.logging.set_verbosity_info()\n",
    "set_seed(42)\n",
    "print(\"start time: \", get_current_time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "32809c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo='/data/all/data/zloirock/core-js'\n",
    "model_name='t5-small'\n",
    "tuned_model_address = './storage/tmp/finetuned/good_402_core-js_1.0/best'\n",
    "general_model_address = './storage/training/t5-small_repo-based_21-01-2022_10-29-42/checkpoint-16440'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5e90d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0eb6fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "COEFF=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "5ebd2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-bs\", \"--batch-size\", type=int, default=32)\n",
    "# parser.add_argument(\"-mn\", \"--model-name\", type=str, choices=[\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"], required=True,)\n",
    "# parser.add_argument(\"-lm\", \"--load-model\", type=str, default=\"\")  # Checkpoint dir to load the model. Example: t5-small_global_14-12-2020_16-29-22/checkpoint-10\n",
    "# parser.add_argument(\"-ea\", \"--eval-all\", type=boolean_string, default=False)  # to evaluate on all data or not\n",
    "# parser.add_argument(\"-eas\", \"--eval-acc-steps\", type=int, default=1)\n",
    "# parser.add_argument(\"-md\", \"--result-dir\", type=str, default=\"\")\n",
    "# parser.add_argument(\"-et\", \"--error-type\", type=str, default=\"\")\n",
    "# parser.add_argument(\"-d\", \"--design\", type=str, required=True, choices=['old', 'new', 'repo-based-included'])\n",
    "# parser.add_argument(\"-r\", \"--repo\", type=str, required=False)\n",
    "# args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d2e98776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "# Create job's directory\n",
    "\n",
    "test_result_directory = f'./test'\n",
    "storage_directory = f'./storage'\n",
    "\n",
    "os.makedirs(test_result_directory, exist_ok=True)\n",
    "with open(os.path.join(test_result_directory, \"commandline_args.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(sys.argv[1:]))\n",
    "\n",
    "# Read data\n",
    "data = GetDataAsPython(f\"{storage_directory}/data_and_models/data/data_autofix_tracking_repo_specific_final.json\")\n",
    "data_eslint = GetDataAsPython(f\"{storage_directory}/data_and_models/data/data_autofix_tracking_eslint_final.json\")\n",
    "data += data_eslint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "bbb18c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104804"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1bfa218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no-invalid-this', 'no-throw-literal', 'no-new-wrappers', 'guard-for-in', 'no-new-object', 'comma-style', 'prefer-spread', 'no-caller', 'no-extra-bind', 'no-array-constructor', 'prefer-rest-params', 'generator-star-spacing', 'no-this-before-super', 'no-extend-native', 'no-undef', 'no-useless-escape', 'no-dupe-keys', 'no-console', 'no-constant-condition', 'no-duplicate-case', 'no-empty', 'no-extra-semi', 'no-redeclare', 'no-cond-assign', 'no-extra-boolean-cast', 'no-fallthrough', 'no-unreachable', 'valid-typeof', 'no-unsafe-finally', 'no-unused-vars', 'no-debugger', 'no-unsafe-negation', 'no-case-declarations', 'no-self-assign', 'no-process-exit', 'no-inner-declarations', 'for-direction', 'no-compare-neg-zero', 'no-sparse-arrays', 'no-func-assign', 'no-const-assign', 'no-global-assign', 'use-isnan', 'no-unused-labels', 'require-yield', 'getter-return', 'no-dupe-class-members', 'no-ex-assign', 'constructor-super', 'no-new-symbol', 'no-empty-pattern', 'no-class-assign']\n",
      "splitting by : repo-based-included\n",
      "train size: 120\n",
      "val size: 40\n",
      "test size: 41\n"
     ]
    }
   ],
   "source": [
    "all_warning_types = extract_warning_types(data)\n",
    "# if args.error_type != \"\":\n",
    "#     all_warning_types = [args.error_type]\n",
    "print(all_warning_types)\n",
    "(train_inputs, train_labels, val_inputs, val_labels, test_inputs, test_labels, train_info, val_info, test_info, ) =\\\n",
    "    create_data(data, all_warning_types, include_warning=True, design='repo-based-included', select_repo=repo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e342030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f04760af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from directory ./storage/training/t5-small_repo-based_21-01-2022_10-29-42/checkpoint-16440\n",
      "Loaded model from directory ./storage/training/t5-small_repo-based_21-01-2022_10-29-42/checkpoint-16440\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer and the model that will be tested.\n",
    "general_tokenizer = T5Tokenizer.from_pretrained(f'{general_model_address}')\n",
    "print(\"Loaded tokenizer from directory {}\".format(f'{general_model_address}'))\n",
    "general_model = T5ForConditionalGeneration.from_pretrained(f'{general_model_address}')\n",
    "print(\"Loaded model from directory {}\".format(f'{general_model_address}'))\n",
    "print(f\"cuda:{torch.cuda.current_device()}\")\n",
    "# general_model.to(f\"cuda:{torch.cuda.current_device()}\")\n",
    "general_model.resize_token_embeddings(len(general_tokenizer))\n",
    "general_model.eval()\n",
    "general_model.to(device)\n",
    "general_model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3346c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from directory ./storage/tmp/finetuned/good_402_core-js_1.0/best\n",
      "Loaded model from directory ./storage/tmp/finetuned/good_402_core-js_1.0/best\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer and the model that will be tested.\n",
    "tuned_tokenizer = T5Tokenizer.from_pretrained(f'{tuned_model_address}')\n",
    "print(\"Loaded tokenizer from directory {}\".format(f'{tuned_model_address}'))\n",
    "tuned_model = T5ForConditionalGeneration.from_pretrained(f'{tuned_model_address}')\n",
    "print(\"Loaded model from directory {}\".format(f'{tuned_model_address}'))\n",
    "print(f\"cuda:{torch.cuda.current_device()}\")\n",
    "# tuned_model.to(f\"cuda:{torch.cuda.current_device()}\")\n",
    "tuned_model.resize_token_embeddings(len(general_tokenizer))\n",
    "tuned_model.eval()\n",
    "tuned_model.to(device)\n",
    "tuned_model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "3efa8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset required by pytorch\n",
    "# general_train_dataset = create_dataset(\n",
    "#     train_inputs, train_labels, general_tokenizer, pad_truncate=True, max_length=128\n",
    "# )\n",
    "# general_val_dataset = create_dataset(val_inputs, val_labels, general_tokenizer, pad_truncate=True)\n",
    "\n",
    "# # Trainer arguments.\n",
    "# # Note that Seq2SeqTrainer class has a method predict() that will be used to generate predictions.\n",
    "# # That is why we still need to create a trainer instance and its arguments even though we are in testing\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=test_result_directory,\n",
    "#     num_train_epochs=0,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     logging_dir=test_result_directory,\n",
    "#     logging_steps=100,\n",
    "#     do_eval=True,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     eval_accumulation_steps=1,  # set this lower, if testing or validation crashes\n",
    "#     predict_with_generate=True,  # never set this to false, it is for testing.\n",
    "#     seed=42,  # default value\n",
    "# )\n",
    "\n",
    "# general_trainer = Seq2SeqTrainer(\n",
    "#     model=general_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=general_train_dataset,\n",
    "#     eval_dataset=general_val_dataset,\n",
    "#     tokenizer=general_tokenizer,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "52771a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing samples:  41\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for key in test_inputs:\n",
    "    counter += len(test_inputs[key])\n",
    "print(\"Number of testing samples: \", counter)\n",
    "\n",
    "# test that the samples are well aligned among inputs and info\n",
    "for warning in test_inputs:\n",
    "    inputs = test_inputs[warning]\n",
    "    infos = test_info[warning]\n",
    "    for i, code in enumerate(inputs):\n",
    "        assert code == infos[i].GetT5Representation(True)[0], \"something wrong! stop it!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "720e2d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule 0 acc: 0.0\n",
      "rule 5 acc: 0.7\n",
      "rule 6 acc: 0.0\n",
      "rule 10 acc: 0.0\n",
      "rule 14 acc: 0.0\n",
      "rule 23 acc: 0.0\n",
      "rule 29 acc: 1.0\n",
      "rule 41 acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "target_max_length = 256  # Set this to 256 if enough memory\n",
    "import gc\n",
    "scores: DefaultDict[str, float] = defaultdict(float)\n",
    "counts: DefaultDict[str, float] = defaultdict(int)\n",
    "for i, warning in enumerate(all_warning_types):\n",
    "    \n",
    "    test_warning = test_inputs[warning]\n",
    "    test_warning_labels = test_labels[warning]\n",
    "    test_warning_info = test_info[warning]\n",
    "    \n",
    "    if not test_warning:\n",
    "        scores[warning] = 'NA'\n",
    "        counts[warning] = 0\n",
    "        continue\n",
    "#     print('coding general')\n",
    "\n",
    "    train_ids = general_tokenizer(\n",
    "        test_warning,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=target_max_length,\n",
    "        ).input_ids\n",
    "    predictions = general_model.generate(train_ids.to(device), max_length=target_max_length, output_scores=True, num_return_sequences=5, num_beams=5, return_dict_in_generate=True)\n",
    "    output_ids = np.pad(\n",
    "        predictions.sequences.cpu(), ((0, 0), (0, target_max_length - predictions.sequences.shape[1])), mode=\"constant\"\n",
    "    )\n",
    "    prediction_scores = predictions.sequences_scores.cpu()\n",
    "    del predictions\n",
    "    gc.collect()\n",
    "#     print('coding tuned')\n",
    "    # print(target_ids.shape)\n",
    "    train_ids_t = tuned_tokenizer(\n",
    "        test_warning,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=target_max_length,\n",
    "    ).input_ids\n",
    "    predictions_tuned = tuned_model.generate(train_ids_t.to(device), max_length=target_max_length, output_scores=True, num_return_sequences=5, num_beams=5, return_dict_in_generate=True)\n",
    "    output_ids_tuned = np.pad(\n",
    "        predictions_tuned.sequences.cpu(), ((0, 0), (0, target_max_length - predictions_tuned.sequences.shape[1])), mode=\"constant\"\n",
    "    )\n",
    "    prediction_scores_tuned = predictions_tuned.sequences_scores.cpu()\n",
    "    \n",
    "    del predictions_tuned\n",
    "#     print(prediction_scores, prediction_scores_tuned)\n",
    "    assert len(output_ids) == 5 * len(test_warning)\n",
    "    all_predictions = []\n",
    "    for j in range(len(test_warning)):\n",
    "        predictions_aggregate = defaultdict(int)\n",
    "        for prediction, score in zip(output_ids[j * 5: (j + 1) * 5], prediction_scores[j * 5: (j + 1) * 5]):\n",
    "            predictions_aggregate[','.join([str(item.item()) for item in prediction])] += score.item() + 1\n",
    "#             print(prediction, score)\n",
    "\n",
    "        for prediction_tuned, score in zip(output_ids_tuned[j * 5: (j + 1) * 5], prediction_scores_tuned[j * 5: (j + 1) * 5]):\n",
    "            predictions_aggregate[','.join([str(item.item()) for item in prediction_tuned])] += COEFF * (score.item() + 1)\n",
    "    #         print(prediction, score)\n",
    "        picked_result = [int(item) for item in max(predictions_aggregate, key=predictions_aggregate.get).split(',')]\n",
    "        all_predictions.append(picked_result)\n",
    "    all_predictions = np.array(all_predictions, dtype=int)\n",
    "        \n",
    "#     target_max_length = 256  # Set this to 256 if enough memory\n",
    "#     if not test_warning:\n",
    "#         scores[warning] = 'NA'\n",
    "#         counts[warning] = 0\n",
    "#         continue\n",
    "#     # print(f\"rule {i}: {warning}, # {len(test_warning)}\")\n",
    "#     test_warning_dataset = create_dataset(\n",
    "#         test_warning,\n",
    "#         test_warning_labels,\n",
    "#         general_tokenizer,\n",
    "#         pad_truncate=True,\n",
    "#         max_length=target_max_length,\n",
    "#     )\n",
    "#     print('decoding')\n",
    "    target_ids = general_tokenizer(\n",
    "        test_warning_labels,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=target_max_length,\n",
    "    ).input_ids\n",
    "    target_ids = np.array(target_ids)\n",
    "\n",
    "#     output_ids = general_trainer.predict(\n",
    "#         test_dataset=test_warning_dataset, num_beams=5, max_length=target_max_length\n",
    "#     ).predictions\n",
    "#     output_ids = np.pad(\n",
    "#         all_predictions, ((0, 0), (0, target_max_length - all_predictions.shape[1])), mode=\"constant\"\n",
    "#     )\n",
    "    output_ids = np.delete(all_predictions, 0, axis=1)\n",
    "    output_ids = np.insert(output_ids, target_max_length - 1, 0, axis=1)\n",
    "\n",
    "    correct_counter = np.sum(np.all(np.equal(target_ids, output_ids), axis=1))\n",
    "    total_counter = len(output_ids)\n",
    "    for k, output_id in enumerate(output_ids):\n",
    "        pred = general_tokenizer.decode(output_id, skip_special_tokens=True)\n",
    "        predictions = []\n",
    "        predictions.append(pred)\n",
    "        test_warning_info[k].predictions = predictions\n",
    "\n",
    "    scores[warning] = correct_counter / total_counter\n",
    "    counts[warning] = total_counter\n",
    "    test_info[warning] = test_warning_info\n",
    "    print(f\"rule {i} acc: {correct_counter / total_counter}\")\n",
    "    predictions_tuned, predictions, output_ids, output_ids_tuned,predictions_aggregate, target_ids, train_ids_t, train_ids = [], [],[],[],[],[],[],[]\n",
    "    pred = []\n",
    "    gc.collect()\n",
    "    \n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "58eee7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for prediction_tuned, score in zip(output_ids_tuned[j * 3: (j + 1) * 3], prediction_scores_tuned[i * 3: (i + 1) * 3]):\n",
    "#     predictions_aggregate[','.join([str(item.item()) for item in prediction_tuned])] += 1.1 * (score.item() + 0.2)\n",
    "#     print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acca004",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a436a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_ids_t == train_ids).sum()/(48 * 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "afa3683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score average: 0.5365853658536586 samples_count: 41\n"
     ]
    }
   ],
   "source": [
    "\n",
    "average, count = get_scores_weighted_average(scores, counts)\n",
    "number_of_warnings = len([scores[k] for k in scores if scores[k] != 'NA'])\n",
    "\n",
    "assert count == counter, 'counts must be equal'\n",
    "\n",
    "scores[\"average\"] = average\n",
    "scores['number_of_warnings'] = number_of_warnings\n",
    "scores['samples_count'] = counter\n",
    "\n",
    "print(f'score average: {average} samples_count: {scores[\"samples_count\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9b26ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{storage_directory}/results.csv', 'a') as f:\n",
    "    f.write(f'ensemble,{repo if repo else \"all\"},{scores[\"average\"]:.2f},{scores[\"number_of_warnings\"]},{scores[\"samples_count\"]},{dt_string},{model_name},{tuned_model_address},{COEFF}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "cd5a6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "bf575c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(target_ids.shape)\n",
    "# train_ids = general_tokenizer(\n",
    "#     test_warning,\n",
    "#     return_tensors=\"pt\",\n",
    "#     truncation=True,\n",
    "#     padding=\"max_length\",\n",
    "#     max_length=target_max_length,\n",
    "# ).input_ids\n",
    "# predictions = general_model.generate(train_ids.to('cuda'), max_length=target_max_length, output_scores=True, num_return_sequences=3, num_beams=5, return_dict_in_generate=True)\n",
    "# prediction_ids = prediction.sequences.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "5b2b3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(target_ids.shape)\n",
    "# train_ids_t = tuned_tokenizer(\n",
    "#     test_warning,\n",
    "#     return_tensors=\"pt\",\n",
    "#     truncation=True,\n",
    "#     padding=\"max_length\",\n",
    "#     max_length=target_max_length,\n",
    "# ).input_ids\n",
    "# predictions_tuned = tuned_model.generate(train_ids.to('cuda'), max_length=target_max_length, output_scores=True, num_return_sequences=3, num_beams=5, return_dict_in_generate=True)\n",
    "# prediction_ids = prediction.sequences.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1ffe0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_ids_t == train_ids).all(), (predictions.sequences == predictions_tuned.sequences).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5c06c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.sequences_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e4ff6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(test_warning)):\n",
    "#     predictions_aggregate = defaultdict(int)\n",
    "#     for prediction, score in zip(predictions.sequences[i * 3: (i + 1) * 3], predictions.sequences_scores[i * 3: (i + 1) * 3]):\n",
    "#         predictions_aggregate[','.join([str(item.item()) for item in prediction])] += score.item() + 0.2\n",
    "# #         print(prediction, score)\n",
    "        \n",
    "#     for prediction_tuned, score in zip(predictions_tuned.sequences[i * 3: (i + 1) * 3], predictions_tuned.sequences_scores[i * 3: (i + 1) * 3]):\n",
    "#         predictions_aggregate[','.join([str(item.item()) for item in prediction_tuned])] += 1.1 * (score.item() + 0.2)\n",
    "# #         print(prediction, score)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "5a7ff63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picked_result = max(predictions_aggregate, key=predictions_aggregate.get).split(',')\n",
    "# picked_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b1303ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.sequences_scores[0:3], predictions_tuned.sequences_scores[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "108d0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_ids = np.pad(\n",
    "#     prediction_ids, ((0, 0), (0, target_max_length - prediction_ids.shape[1])), mode=\"constant\"\n",
    "# )\n",
    "# prediction_ids = np.delete(prediction_ids, 0, axis=1)\n",
    "# prediction_ids = np.insert(prediction_ids, target_max_length - 1, 0, axis=1)\n",
    "# o_prediction = general_trainer.predict(test_dataset=test_warning_dataset, num_beams=5, max_length=target_max_length)\n",
    "# output_ids = o_prediction.predictions\n",
    "# output_ids = np.pad(\n",
    "#     output_ids, ((0, 0), (0, target_max_length - output_ids.shape[1])), mode=\"constant\"\n",
    "# )\n",
    "# output_ids = np.delete(output_ids, 0, axis=1)\n",
    "# output_ids = np.insert(output_ids, target_max_length - 1, 0, axis=1)\n",
    "# (output_ids == prediction_ids).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c0ec79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_ids = np.pad(\n",
    "#     prediction_ids, ((0, 0), (0, target_max_length - prediction_ids.shape[1])), mode=\"constant\"\n",
    "# )\n",
    "# prediction_ids = np.delete(prediction_ids, 0, axis=1)\n",
    "# prediction_ids = np.insert(prediction_ids, target_max_length - 1, 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a5a14497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general_model.generate(train_ids.to('cuda'), max_length=target_max_length, output_scores=True, num_return_sequences=1, num_beams=5).sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "fbb3cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_counter = np.sum(np.all(np.equal(target_ids, output_ids), axis=1))\n",
    "# total_counter = len(output_ids)\n",
    "# for k, output_id in enumerate(output_ids):\n",
    "#     pred = general_tokenizer.decode(output_id, skip_special_tokens=True)\n",
    "#     predictions = []\n",
    "#     predictions.append(pred)\n",
    "#     test_warning_info[k].predictions = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "d9fcfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores[warning] = correct_counter / total_counter\n",
    "# counts[warning] = total_counter\n",
    "# test_info[warning] = test_warning_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "870f1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee7734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a28b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e25870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c52a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5de7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe901e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32093f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350be872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03386141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5ab5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a1330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835b853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
