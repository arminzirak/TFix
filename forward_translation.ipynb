{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4408fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1adeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_THRESHHOLD = 0\n",
    "FW_EPOCHS=1\n",
    "repo = '/data/all/data/emberjs/ember.js'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f3ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./hf_transformers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c260cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import T5Config\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d9db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c12af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data_reader import GetDataAsPython\n",
    "from prepare_data import create_data\n",
    "from prepare_data import create_dataset\n",
    "from prepare_data import extract_warning_types\n",
    "from utils import boolean_string\n",
    "from utils import get_current_time\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a3bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    storage_directory = './storage/'\n",
    "    base_model = f'./{storage_directory}/training//t5-small_repo-based_21-01-2022_10-29-42/checkpoint-16440'\n",
    "    batch_size = 16\n",
    "#     codebert_address = \"microsoft/codebert-base\"\n",
    "else:\n",
    "    storage_directory = '/scratch/arminz/'\n",
    "    batch_size = 64\n",
    "    # base_model = f'/{storage_directory}/t5-small_global_repo-based_03-11-2021_15-28-40/checkpoint-37375/'\n",
    "    base_model = f'{storage_directory}/training/checkpoint-37375'\n",
    "#     codebert_address = \"/home/arminz/codebert-base\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9714988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import codebert_utils\n",
    "# codebert_utils.load(codebert_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d6e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_number = random.randint(0, 1000)\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-a\", \"--append\", type=int, required=True)\n",
    "# parser.add_argument(\"-rp\", \"--repo_percent\", type=float, required=True)\n",
    "# parser.add_argument(\"-r\", \"--repo\", type=str, required=True)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# append = 56*1000#args.append\n",
    "# repo = '/data/all/data/emberjs/ember.js' #,/data/all/data/request/request'#, args.repo\n",
    "# repo_percent = 0 # args.repo_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980ef80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104804"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = GetDataAsPython(f\"{storage_directory}/data_and_models/data/data_autofix_tracking_repo_specific_final.json\")\n",
    "data_eslint = GetDataAsPython(f\"{storage_directory}/data_and_models/data/data_autofix_tracking_eslint_final.json\")\n",
    "data += data_eslint\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d7a298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/all/data/xpl/useless'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d41207f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104804"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "722ce948",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ft1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2ed81d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting by : repo-based-included\n",
      "train size: 129\n",
      "val size: 42\n",
      "test size: 47\n"
     ]
    }
   ],
   "source": [
    "all_warning_types = extract_warning_types(data)\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "(repo_train_inputs, repo_train_labels, repo_val_inputs, repo_val_labels, repo_test_inputs, repo_test_labels,\n",
    " repo_train_info, repo_val_info, repo_test_info,) = create_data(data, all_warning_types, include_warning=True,\n",
    "                                                                design='repo-based-included', select_repo=repo)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5033c7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armin/TFix/env/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:185: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./storage//tmp/ft1_16_ember.js_0_1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(base_model)\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "train_dataset = create_dataset(repo_train_inputs, repo_train_labels, tokenizer, pad_truncate=True, max_length=128)\n",
    "val_dataset = create_dataset(repo_val_inputs, repo_val_labels, tokenizer, pad_truncate=True)\n",
    "# test_dataset = create_dataset(repo_val_inputs, repo_val_labels, tokenizer, pad_truncate=True)\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "test_result_directory = f'{storage_directory}/fine-tune-result'\n",
    "full_name = f'{name}_{exec_number}_{repo.rsplit(\"/\", 1)[1][-20:]}_{SCORE_THRESHHOLD}_{FW_EPOCHS}'\n",
    "model_directory = f'{storage_directory}/tmp/{full_name}'\n",
    "model_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51dc8426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17752a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3#4e-3\n",
    "ws = 300\n",
    "wd = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc7c0133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32104, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32104, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32104, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32104, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(base_model)\n",
    "model = T5ForConditionalGeneration.from_pretrained(base_model)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c191145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)#, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size)#, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daf11ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0\n",
      "8.650285684419469e-21 0.9981783032417297 0.4020351200785978\n",
      "129 0.4020351200785978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='45' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 45/135 00:12 < 00:26, 3.41 it/s, Epoch 5/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.113078</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>369.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.112026</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>369.926000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111405</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>372.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.112825</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>367.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.115915</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>380.477000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "for fw_epoch in range(FW_EPOCHS):\n",
    "    if fw_epoch > 0:\n",
    "        model = T5ForConditionalGeneration.from_pretrained('./tmp_test_model').to('cuda')\n",
    "    \n",
    "    print('---- ' + str(fw_epoch))\n",
    "    predictions_all = []\n",
    "    scores = []\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "    #     loss = outputs[0]#outputs.loss\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     lr_scheduler.step()\n",
    "    #     optimizer.zero_grad()\n",
    "    #     progress_bar.update(1)\n",
    "\n",
    "        predictions = outputs[1].argmax(-1)\n",
    "        for prediction in predictions:\n",
    "            decoded = tokenizer.decode(prediction)\n",
    "            predictions_all.append(decoded[:decoded.find('<pad>')])\n",
    "        scores += [item.item() for item in softmax(outputs[1], dim=-1).max(-1).values.prod(-1).to('cpu')]  \n",
    "        \n",
    "    print(min(scores), max(scores), sum(scores) / len(scores))\n",
    "    \n",
    "    filterred_repo_train_inputs, filterred_predictions, filtered_scores = list(), list(), list()\n",
    "    for repo_train_input, prediction, score in zip(repo_train_inputs, predictions_all, scores):\n",
    "        if score > SCORE_THRESHHOLD:\n",
    "            filterred_repo_train_inputs.append(repo_train_input)\n",
    "            filterred_predictions.append(prediction)\n",
    "            filtered_scores.append(score)\n",
    "    print(len(filtered_scores), sum(filtered_scores) / len(filtered_scores))\n",
    "    \n",
    "    created_tune_dataset = create_dataset(filterred_repo_train_inputs, filterred_predictions, tokenizer, pad_truncate=True, max_length=128)    \n",
    "    \n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=model_directory,\n",
    "        num_train_epochs=15,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=ws,\n",
    "        weight_decay=wd,\n",
    "        logging_dir=model_directory,\n",
    "        logging_steps=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=lr,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=1,\n",
    "        eval_accumulation_steps=1,  # set this lower, if testing or validation crashes\n",
    "        disable_tqdm=False,\n",
    "        predict_with_generate=True,  # never set this to false.\n",
    "        seed=42,  # default value\n",
    "    )\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=created_tune_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        optimizers=[torch.optim.Adam(params=model.parameters(), lr=lr), None],\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        #     compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model('./tmp_test_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "555e05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predictions_all = []\n",
    "# scores = []\n",
    "# for batch in train_dataloader:\n",
    "#     batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "#     outputs = model(**batch)\n",
    "# #     loss = outputs[0]#outputs.loss\n",
    "# #     loss.backward()\n",
    "# #     optimizer.step()\n",
    "# #     lr_scheduler.step()\n",
    "# #     optimizer.zero_grad()\n",
    "# #     progress_bar.update(1)\n",
    "\n",
    "#     predictions = outputs[1].argmax(-1)\n",
    "#     for prediction in predictions:\n",
    "#         decoded = tokenizer.decode(prediction)\n",
    "#         predictions_all.append(decoded[:decoded.find('<pad>')])\n",
    "#         scores += [item.item() for item in softmax(outputs[1], dim=-1).max(-1).values.prod(-1).to('cpu')]         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6937465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.boxplot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05884d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('keyword = params[0].to; contextvar localizedOptions = o_create(options);</s> ',\n",
       " \"fix no-invalid-this Unexpected 'this'.     context = this.get('context');\\n:\\n    keyword = params[0].to;\\n    context = this.get('context');\\n\\n    var localizedOptions = o_create(options);\\n </s>\",\n",
       " '    keyword = params[0].to;\\n\\n    var localizedOptions = o_create(options);\\n </s>')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode(predictions)\n",
    "# predictions.shape\n",
    "ind = 15\n",
    "predictions_all[ind], repo_train_inputs[ind], repo_train_labels[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09dbe797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filttered_repo_train_inputs, filterred_predictions, filttered_scores = list(), list(), list()\n",
    "# for repo_train_input, prediction, score in zip(repo_train_inputs, predictions_all, scores):\n",
    "#     if score > 0.4:\n",
    "#         filttered_repo_train_inputs.append(repo_train_input)\n",
    "#         filterred_predictions.append(prediction)\n",
    "#         filttered_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e2ec249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created_tune_dataset = create_dataset(filttered_repo_train_inputs, filterred_predictions, tokenizer, pad_truncate=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6eafdc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import EarlyStoppingCallback\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=model_directory,\n",
    "#     num_train_epochs=15,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     warmup_steps=ws,\n",
    "#     weight_decay=wd,\n",
    "#     logging_dir=model_directory,\n",
    "#     logging_steps=100,\n",
    "#     do_eval=True,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=lr,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"eval_loss\",\n",
    "#     greater_is_better=False,\n",
    "#     save_total_limit=1,\n",
    "#     eval_accumulation_steps=1,  # set this lower, if testing or validation crashes\n",
    "#     disable_tqdm=False,\n",
    "#     predict_with_generate=True,  # never set this to false.\n",
    "#     seed=42,  # default value\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e0fc59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=created_tune_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     optimizers=[torch.optim.Adam(params=model.parameters(), lr=lr), None],\n",
    "#     tokenizer=tokenizer,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    "#     #     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6642f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f920956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 0.1137516126036644\n"
     ]
    }
   ],
   "source": [
    "print('eval', trainer.evaluate()['eval_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b94fa5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model dir ./storage//tmp/ft_626_ember.js_0.6_5/best/\n"
     ]
    }
   ],
   "source": [
    "best_model_dir = f'{model_directory}/best/'\n",
    "trainer.save_model(best_model_dir)\n",
    "print('best model dir', best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2a4b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e091f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armin/TFix/env/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:185: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "12it [00:07,  2.04it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:  01:57:35\n",
      "['no-invalid-this', 'no-throw-literal', 'no-new-wrappers', 'guard-for-in', 'no-new-object', 'comma-style', 'prefer-spread', 'no-caller', 'no-extra-bind', 'no-array-constructor', 'prefer-rest-params', 'generator-star-spacing', 'no-this-before-super', 'no-extend-native', 'no-undef', 'no-useless-escape', 'no-dupe-keys', 'no-console', 'no-constant-condition', 'no-duplicate-case', 'no-empty', 'no-extra-semi', 'no-redeclare', 'no-cond-assign', 'no-extra-boolean-cast', 'no-fallthrough', 'no-unreachable', 'valid-typeof', 'no-unsafe-finally', 'no-unused-vars', 'no-debugger', 'no-unsafe-negation', 'no-case-declarations', 'no-self-assign', 'no-process-exit', 'no-inner-declarations', 'for-direction', 'no-compare-neg-zero', 'no-sparse-arrays', 'no-func-assign', 'no-const-assign', 'no-global-assign', 'use-isnan', 'no-unused-labels', 'require-yield', 'getter-return', 'no-dupe-class-members', 'no-ex-assign', 'constructor-super', 'no-new-symbol', 'no-empty-pattern', 'no-class-assign']\n",
      "splitting by : repo-based-included\n",
      "train size: 129\n",
      "val size: 42\n",
      "test size: 47\n",
      "Loaded tokenizer from directory ./storage//tmp/ft_626_ember.js_0.6_5/best/\n",
      "Loaded model from directory ./storage//tmp/ft_626_ember.js_0.6_5/best/\n",
      "cuda:0\n",
      "Testing has started\n",
      "Number of testing samples:  47\n",
      "score average: 0.5106382978723404 samples_count: 47\n",
      "result : ./storage//testing/5/per-repo/t5-small_test_ember.js_05-03-2022_01-57-35\n",
      "end time:  01:57:54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.system(\n",
    "    f'python hf_transformers/tfix_testing.py --load-model {best_model_dir} -bs {batch_size} --model-name t5-small -d repo-based-included -r {repo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0beb2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "672bc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63a20ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('./tmp_test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d6714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510e398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048475a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aebb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca8be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22fc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0f0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfe034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0473a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a11f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f10fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}