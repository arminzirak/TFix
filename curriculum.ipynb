{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2cd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3653674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import T5Config\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import set_seed\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"./hf_transformers/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b62dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data_reader import GetDataAsPython\n",
    "from prepare_data import create_data\n",
    "from prepare_data import create_dataset\n",
    "from prepare_data import extract_warning_types\n",
    "from utils import boolean_string\n",
    "from utils import get_current_time\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from data_reader import GetDataAsPython\n",
    "from prepare_data import create_data\n",
    "from prepare_data import create_dataset\n",
    "from prepare_data import extract_warning_types\n",
    "from utils import boolean_string\n",
    "from utils import get_current_time\n",
    "import csv\n",
    "\n",
    "start_all = datetime.now()\n",
    "\n",
    "# In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc64c9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local = True\n",
    "base_model = 'training/t5-small_repo-based_21-01-2022_10-29-42/checkpoint-16440'\n",
    "\n",
    "if local:\n",
    "    storage_directory = './storage/'\n",
    "    load_model = f'./{storage_directory}/{base_model}'\n",
    "    batch_size = 16\n",
    "else:\n",
    "    storage_directory = '/scratch/arminz/'\n",
    "    batch_size = 64\n",
    "    load_model = f'/{storage_directory}/{base_model}'\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "exec_number = random.randint(0, 1000)\n",
    "exec_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed201ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: /data/all/data/elastic/kibana 1\n",
      "best arguments 0.004 0.4 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104804"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-r\", \"--repo\", type=str, default='/data/all/data/oroinc/platform')\n",
    "# parser.add_argument(\"-p\", \"--percent\", type=float, default=1)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "repo = \"/data/all/data/elastic/kibana\" #args.repo\n",
    "sample_percent = 1#args.percent\n",
    "\n",
    "print('start:', repo, sample_percent)\n",
    "\n",
    "lr = 4e-3\n",
    "ws = 300\n",
    "wd = 0.4\n",
    "print('best arguments', lr, wd, ws)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "name='curr'\n",
    "name\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "# Read and prepare data\n",
    "data = GetDataAsPython(f\"{storage_directory}/data_and_models/data/data_autofix_tracking_repo_specific_final.json\")\n",
    "data_eslint = GetDataAsPython(f\"{storage_directory}/data_and_models/data/data_autofix_tracking_eslint_final.json\")\n",
    "data += data_eslint\n",
    "\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160698b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting by : repo-based-included\n",
      "train size: 136\n",
      "val size: 44\n",
      "test size: 51\n",
      "amount of data that is being used for fine-tuning (train) : 136 == 136 (1)\n",
      "amount of data that is being used for fine-tuning (validation): 44 (full)\n",
      "amount of data that will be probably being used for testing: 51 (full)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armin/TFix/env/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:185: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_warning_types = extract_warning_types(data)\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "(train_inputs, train_labels, val_inputs, val_labels, test_inputs, test_labels, train_info, val_info, test_info, ) = \\\n",
    "    create_data(data, all_warning_types, include_warning=True, design='repo-based-included', select_repo=repo)\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(load_model)\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "len(train_inputs)\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "\n",
    "# Create dataset required by pytorch\n",
    "samples = int(sample_percent * len(train_inputs))\n",
    "train_dataset = create_dataset(train_inputs[:samples], train_labels[:samples], tokenizer, pad_truncate=True, max_length=128)\n",
    "val_dataset = create_dataset(val_inputs, val_labels, tokenizer, pad_truncate=True)\n",
    "\n",
    "print(f'amount of data that is being used for fine-tuning (train) : {len(train_dataset)} == {samples} ({sample_percent})')\n",
    "print(f'amount of data that is being used for fine-tuning (validation): {len(val_dataset)} (full)')\n",
    "print(f'amount of data that will be probably being used for testing: {sum([len(x) for x in test_inputs.values()])} (full)')\n",
    "\n",
    "# In[61]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac60d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_data.BugFixDataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d183f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91695e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "# checkpoint = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "# def tokenize_function(example):\n",
    "#     return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "# tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355e5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler, SequentialSampler\n",
    "from typing import Sized, Iterator\n",
    "class MySampler(Sampler[int]):\n",
    "    data_source: Sized\n",
    "    \n",
    "    def __init__(self, data_source: Sized) -> None:\n",
    "        self.data_source = data_source\n",
    "        self.priority =  np.zeros(len(data_source))\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        return reversed(np.argsort(self.priority))\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n",
    "\n",
    "    def set_priority(self, priority):\n",
    "        self.priority = priority\n",
    "        \n",
    "sampler = MySampler(train_dataset)\n",
    "# list(MySampler([1, 2, 3, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505a8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)#, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size)#, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed44ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader.sampler.set_priority(list(range(len(train_dataset) - 1)) + [-5])\n",
    "# for ind, batch in train_dataloader:\n",
    "#     print(ind)\n",
    "# {k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3598ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15579a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./storage//tmp/finetuned/curr_118_kibana_1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "now = datetime.now()\n",
    "full_name = f'{name}_{exec_number}_{repo.rsplit(\"/\", 1)[1][-20:]}_{sample_percent}'\n",
    "model_directory = f'{storage_directory}/tmp/finetuned/{full_name}'\n",
    "model_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e6e693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32104, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32104, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32104, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32104, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(load_model)\n",
    "model = T5ForConditionalGeneration.from_pretrained(load_model)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8795d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed68d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = np.array(outputs[1].argmax(-1).to('cpu'))\n",
    "# labels = np.array(batch['labels'].to('cpu'))\n",
    "# np.sum(np.all(np.equal(predictions, labels), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6257170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8fca80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "838f4fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_training_steps = num_train_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=ws,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7461a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d9bff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, batch in train_dataloader:\n",
    "#     batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "#     print(index)#, batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9864a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [value.item() for key, value in sorted(list(zip(list(sampler)[:16], outputs[1].max(-1).values.sum(1))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "798e23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "scores = []\n",
    "for batch in train_dataloader:\n",
    "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    scores += [item.item() for item in outputs[1].max(-1).values.mean(1).to('cpu')]\n",
    "    \n",
    "# print(f'epoch #{epoch} | loss: {loss:.2f}, accuracy : {all_corrects/ all_cnt:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f414125e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38046014308929443,\n",
       " 0.7871943116188049,\n",
       " 0.5865635871887207,\n",
       " 0.0036719662602990866,\n",
       " 0.2610129415988922,\n",
       " 0.689932107925415,\n",
       " 0.36043503880500793,\n",
       " 0.00020400254288688302]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "[item.item() for item in softmax(outputs[1], dim=-1).max(-1).values.prod(-1).to('cpu')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4011480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4b734aff42441eb8c8bc059c72f6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111, 113, 101, 105, 85, 77, 74, 69, 79, 65, 62, 75, 64, 88, 87, 83, 84, 68, 73, 78, 70, 86, 106, 91, 89, 76, 80, 90, 58, 81, 72, 63, 82, 67, 125, 124, 56, 55, 53, 122, 107, 31, 66, 127, 128, 71, 54, 94, 134, 112, 35, 120, 109, 98, 121, 95, 33, 3, 7, 12, 15, 32, 123, 24, 9, 36, 8, 92, 108, 93, 37, 18, 99, 115, 38, 114, 100, 40, 97, 19, 110, 39, 130, 103, 20, 131, 17, 51, 59, 116, 14, 27, 26, 119, 104, 133, 29, 135, 2, 28, 13, 60, 132, 34, 102, 126, 5, 10, 117, 118, 21, 16, 23, 1, 57, 6, 50, 25, 30, 41, 11, 96, 46, 49, 45, 52, 42, 48, 22, 0, 4, 43, 129, 47, 61, 44]\n",
      "---\n",
      "epoch #0 | tr_loss:0.73 tr_acc:0.3602941176470588 val_loss: 0.12, val_accuracy: 0.455\n",
      "[111, 113, 101, 105, 75, 88, 74, 79, 62, 85, 77, 87, 69, 90, 65, 89, 64, 83, 84, 78, 73, 80, 68, 70, 91, 86, 76, 81, 106, 58, 82, 63, 72, 67, 56, 55, 53, 122, 31, 66, 107, 127, 128, 71, 54, 124, 120, 94, 134, 33, 109, 35, 125, 123, 98, 7, 112, 15, 24, 95, 92, 9, 8, 37, 36, 121, 108, 18, 3, 99, 40, 12, 32, 38, 115, 100, 97, 114, 110, 19, 93, 17, 131, 103, 20, 130, 39, 116, 29, 14, 104, 51, 102, 119, 59, 26, 135, 27, 34, 60, 126, 133, 2, 28, 5, 13, 132, 117, 10, 118, 23, 16, 1, 57, 21, 6, 25, 50, 30, 41, 46, 96, 11, 49, 45, 42, 52, 48, 0, 4, 22, 43, 129, 47, 44, 61]\n",
      "---\n",
      "epoch #1 | tr_loss:0.60 tr_acc:0.4338235294117647 val_loss: 0.11, val_accuracy: 0.523\n",
      "[111, 113, 125, 88, 74, 75, 77, 62, 90, 87, 89, 85, 79, 65, 69, 64, 78, 80, 83, 84, 91, 70, 73, 68, 81, 105, 86, 101, 76, 58, 82, 63, 72, 106, 67, 31, 56, 55, 122, 120, 66, 107, 127, 128, 124, 53, 71, 54, 94, 134, 109, 35, 12, 3, 32, 112, 121, 33, 123, 92, 15, 7, 24, 98, 37, 8, 108, 99, 38, 40, 29, 95, 18, 14, 131, 114, 100, 19, 116, 36, 20, 10, 5, 26, 13, 110, 17, 27, 16, 93, 9, 132, 6, 97, 1, 115, 59, 126, 21, 23, 119, 39, 117, 34, 102, 130, 60, 103, 28, 51, 2, 104, 57, 135, 30, 133, 118, 25, 50, 41, 46, 45, 52, 96, 49, 11, 48, 42, 22, 0, 4, 43, 129, 47, 44, 61]\n",
      "---\n",
      "epoch #2 | tr_loss:0.49 tr_acc:0.49264705882352944 val_loss: 0.14, val_accuracy: 0.545\n",
      "[125, 113, 111, 88, 75, 74, 62, 89, 90, 87, 69, 77, 85, 79, 64, 65, 78, 70, 83, 91, 84, 80, 101, 68, 73, 105, 81, 86, 76, 58, 82, 63, 106, 72, 31, 67, 122, 56, 120, 55, 66, 107, 124, 128, 94, 127, 12, 54, 32, 3, 71, 98, 33, 95, 112, 15, 134, 35, 92, 29, 121, 7, 109, 20, 26, 14, 27, 131, 38, 116, 16, 19, 8, 1, 9, 21, 123, 10, 6, 37, 108, 5, 40, 114, 99, 100, 13, 53, 24, 110, 117, 17, 126, 39, 130, 115, 132, 23, 93, 2, 36, 28, 104, 18, 57, 97, 34, 103, 133, 60, 119, 30, 51, 59, 135, 102, 25, 118, 50, 52, 41, 46, 45, 49, 11, 42, 48, 96, 0, 22, 4, 129, 43, 44, 47, 61]\n",
      "---\n",
      "epoch #3 | tr_loss:0.42 tr_acc:0.5367647058823529 val_loss: 0.13, val_accuracy: 0.477\n",
      "[125, 113, 111, 105, 101, 106, 70, 88, 75, 89, 62, 87, 74, 90, 91, 79, 58, 69, 65, 64, 76, 78, 83, 85, 84, 77, 68, 73, 80, 81, 63, 86, 82, 72, 56, 67, 98, 31, 120, 107, 55, 122, 94, 66, 124, 7, 12, 3, 32, 128, 127, 95, 15, 33, 20, 29, 92, 1, 54, 14, 26, 10, 21, 27, 6, 5, 109, 116, 134, 13, 131, 40, 53, 71, 19, 38, 16, 130, 112, 114, 100, 35, 108, 9, 117, 24, 104, 8, 126, 110, 121, 123, 93, 37, 99, 51, 17, 30, 18, 59, 39, 115, 133, 2, 103, 135, 28, 132, 36, 23, 102, 97, 50, 57, 60, 34, 119, 118, 52, 25, 48, 46, 45, 41, 42, 49, 11, 0, 96, 22, 4, 43, 47, 129, 44, 61]\n",
      "---\n",
      "epoch #4 | tr_loss:0.33 tr_acc:0.5441176470588235 val_loss: 0.12, val_accuracy: 0.523\n",
      "[125, 113, 105, 101, 111, 31, 58, 106, 89, 62, 90, 79, 56, 75, 88, 87, 65, 64, 74, 84, 70, 78, 85, 63, 69, 91, 55, 76, 80, 81, 68, 83, 73, 86, 77, 98, 82, 72, 67, 15, 7, 120, 122, 53, 66, 1, 12, 10, 40, 127, 128, 32, 3, 92, 20, 29, 21, 6, 5, 27, 14, 94, 95, 124, 13, 26, 107, 33, 116, 54, 19, 99, 123, 108, 109, 130, 37, 117, 103, 121, 126, 71, 59, 110, 8, 134, 24, 16, 112, 38, 9, 36, 39, 114, 131, 35, 97, 100, 18, 104, 93, 51, 133, 23, 17, 115, 135, 28, 60, 2, 30, 57, 46, 45, 132, 119, 34, 0, 118, 48, 50, 102, 11, 52, 41, 42, 49, 25, 96, 22, 47, 4, 129, 43, 61, 44]\n",
      "---\n",
      "epoch #5 | tr_loss:0.27 tr_acc:0.5588235294117647 val_loss: 0.17, val_accuracy: 0.477\n",
      "[125, 113, 105, 111, 98, 101, 7, 106, 58, 5, 21, 10, 1, 90, 62, 84, 89, 81, 64, 88, 15, 82, 74, 65, 75, 78, 68, 80, 91, 79, 70, 87, 31, 73, 85, 63, 83, 77, 86, 56, 55, 69, 6, 120, 72, 76, 20, 13, 107, 26, 67, 29, 122, 14, 66, 92, 27, 124, 94, 3, 32, 130, 40, 95, 12, 16, 19, 127, 108, 128, 33, 103, 37, 126, 123, 54, 99, 38, 121, 133, 114, 23, 100, 131, 8, 117, 112, 115, 110, 53, 9, 134, 39, 93, 17, 109, 24, 116, 36, 18, 51, 104, 35, 97, 46, 59, 34, 50, 71, 0, 11, 60, 57, 48, 30, 25, 45, 2, 118, 119, 28, 102, 132, 41, 135, 52, 42, 49, 4, 22, 47, 96, 43, 129, 61, 44]\n",
      "---\n",
      "epoch #6 | tr_loss:0.24 tr_acc:0.5808823529411765 val_loss: 0.17, val_accuracy: 0.545\n",
      "[125, 113, 7, 105, 101, 15, 10, 111, 5, 55, 1, 94, 107, 21, 6, 13, 70, 58, 91, 63, 79, 76, 89, 87, 88, 62, 75, 72, 65, 20, 90, 74, 56, 86, 80, 73, 69, 64, 77, 84, 85, 26, 83, 78, 68, 81, 82, 29, 14, 16, 120, 27, 95, 67, 130, 98, 33, 31, 99, 92, 106, 121, 122, 108, 131, 134, 66, 40, 53, 19, 23, 126, 100, 37, 114, 112, 38, 127, 115, 39, 8, 93, 117, 110, 32, 71, 133, 12, 128, 24, 48, 3, 116, 123, 46, 135, 17, 45, 109, 51, 54, 97, 0, 11, 104, 30, 28, 124, 34, 50, 2, 102, 25, 132, 103, 60, 35, 119, 9, 36, 18, 57, 118, 49, 59, 52, 41, 42, 44, 4, 96, 22, 43, 47, 129, 61]\n",
      "---\n",
      "epoch #7 | tr_loss:0.18 tr_acc:0.4852941176470588 val_loss: 0.15, val_accuracy: 0.432\n",
      "[98, 113, 10, 105, 125, 101, 5, 7, 53, 31, 13, 1, 70, 91, 58, 89, 87, 62, 79, 75, 69, 56, 84, 88, 74, 83, 90, 65, 85, 111, 80, 76, 77, 86, 68, 81, 73, 64, 21, 63, 78, 82, 72, 20, 14, 29, 26, 107, 6, 27, 120, 66, 55, 106, 38, 134, 130, 92, 67, 122, 128, 112, 93, 23, 108, 37, 95, 126, 16, 12, 33, 32, 40, 3, 15, 94, 127, 109, 99, 8, 48, 110, 24, 19, 114, 97, 133, 123, 135, 100, 71, 9, 46, 115, 17, 34, 45, 117, 54, 0, 39, 131, 116, 57, 25, 121, 28, 119, 49, 2, 59, 42, 11, 132, 35, 124, 36, 51, 30, 50, 104, 103, 18, 60, 44, 22, 118, 102, 52, 41, 96, 4, 47, 129, 43, 61]\n",
      "---\n",
      "epoch #8 | tr_loss:0.21 tr_acc:0.4338235294117647 val_loss: 0.17, val_accuracy: 0.455\n",
      "[5, 92, 10, 1, 31, 21, 105, 6, 98, 13, 55, 113, 94, 101, 58, 106, 7, 120, 69, 62, 89, 53, 87, 75, 91, 74, 20, 63, 83, 80, 77, 90, 56, 65, 76, 85, 70, 88, 84, 86, 79, 64, 112, 68, 81, 78, 111, 82, 27, 26, 73, 15, 29, 16, 123, 107, 130, 14, 67, 40, 33, 122, 121, 24, 134, 131, 66, 125, 71, 135, 23, 93, 97, 128, 115, 99, 48, 127, 72, 39, 108, 8, 117, 126, 119, 49, 109, 50, 32, 19, 11, 114, 3, 100, 34, 95, 110, 46, 45, 38, 57, 12, 9, 2, 28, 51, 104, 17, 102, 44, 59, 0, 60, 25, 35, 18, 37, 22, 42, 30, 132, 54, 103, 36, 116, 96, 52, 118, 124, 133, 129, 41, 43, 4, 47, 61]\n",
      "---\n",
      "epoch #9 | tr_loss:0.27 tr_acc:0.45588235294117646 val_loss: 0.19, val_accuracy: 0.409\n",
      "[92, 31, 105, 40, 10, 56, 21, 5, 1, 107, 120, 113, 13, 53, 58, 87, 89, 74, 6, 62, 75, 90, 83, 64, 80, 88, 85, 77, 91, 84, 73, 86, 69, 79, 81, 63, 68, 106, 70, 65, 94, 78, 76, 122, 82, 112, 29, 67, 72, 135, 66, 98, 27, 130, 7, 16, 15, 125, 131, 46, 26, 127, 23, 97, 33, 101, 123, 45, 108, 121, 100, 128, 119, 134, 126, 111, 109, 55, 39, 115, 20, 114, 104, 95, 17, 14, 0, 49, 93, 2, 110, 50, 3, 54, 25, 99, 28, 12, 71, 59, 19, 24, 32, 44, 51, 38, 34, 8, 117, 96, 11, 37, 133, 48, 35, 18, 42, 116, 102, 52, 118, 129, 22, 43, 9, 36, 124, 57, 132, 60, 103, 30, 47, 4, 41, 61]\n",
      "---\n",
      "epoch #10 | tr_loss:0.17 tr_acc:0.5367647058823529 val_loss: 0.24, val_accuracy: 0.386\n",
      "[98, 92, 40, 53, 94, 105, 7, 113, 101, 21, 1, 31, 5, 13, 10, 69, 87, 90, 83, 56, 62, 75, 89, 80, 85, 77, 74, 88, 84, 73, 64, 91, 79, 81, 107, 120, 76, 63, 65, 78, 82, 70, 86, 68, 58, 95, 106, 112, 6, 67, 125, 72, 15, 135, 122, 111, 38, 66, 130, 23, 131, 134, 121, 97, 33, 49, 2, 28, 128, 45, 32, 46, 48, 3, 127, 24, 37, 123, 12, 115, 100, 39, 20, 109, 104, 117, 50, 96, 114, 108, 16, 59, 99, 11, 110, 17, 126, 71, 8, 132, 116, 51, 0, 52, 19, 44, 34, 119, 9, 102, 55, 14, 133, 25, 60, 42, 22, 93, 129, 26, 61, 35, 57, 27, 18, 36, 29, 103, 43, 54, 30, 4, 124, 118, 41, 47]\n",
      "---\n",
      "epoch #11 | tr_loss:0.13 tr_acc:0.4485294117647059 val_loss: 0.22, val_accuracy: 0.455\n",
      "[92, 111, 94, 125, 105, 107, 10, 40, 1, 69, 75, 89, 90, 87, 62, 74, 79, 83, 85, 86, 64, 80, 88, 77, 76, 65, 84, 91, 2, 70, 63, 82, 28, 81, 98, 73, 68, 78, 21, 58, 13, 5, 67, 130, 101, 135, 6, 38, 95, 29, 121, 53, 72, 66, 33, 45, 20, 113, 134, 46, 27, 106, 14, 23, 26, 50, 108, 131, 31, 0, 127, 128, 109, 112, 12, 15, 32, 126, 96, 24, 123, 3, 114, 56, 104, 49, 117, 11, 9, 100, 133, 7, 115, 19, 16, 25, 52, 34, 17, 122, 37, 97, 48, 57, 110, 120, 102, 99, 124, 51, 71, 116, 59, 44, 55, 129, 8, 22, 93, 42, 41, 39, 36, 119, 43, 54, 132, 103, 18, 35, 30, 60, 118, 4, 61, 47]\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #12 | tr_loss:0.17 tr_acc:0.5441176470588235 val_loss: 0.25, val_accuracy: 0.318\n",
      "terminating... using 2\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_train_epochs * len(train_dataloader)))\n",
    "\n",
    "model.train()\n",
    "best_val_accuracy, best_val_loss = 0, 1\n",
    "patience = 10\n",
    "best_model = copy.deepcopy(model)\n",
    "no_imp = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "#         scores += [item.item() for item in outputs[1].max(-1).values.mean(1).to('cpu')]\n",
    "        scores += [item.item() for item in softmax(outputs[1], dim=-1).max(-1).values.prod(-1).to('cpu')]\n",
    "    new_priorities = [value for key, value in sorted(list(zip(list(sampler), scores)))]    \n",
    "    sampler.set_priority(new_priorities)\n",
    "    print(list(sampler))\n",
    "    print('---')\n",
    "    \n",
    "    model.train()\n",
    "    all_corrects, all_cnt = 0, 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]#outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        predictions = np.array(outputs[1].argmax(-1).to('cpu'))\n",
    "        labels = np.array(batch['labels'].to('cpu'))\n",
    "        corrects = np.sum(np.all(np.equal(predictions, labels), axis=1))\n",
    "        \n",
    "        all_cnt += len(batch['labels'])\n",
    "        all_corrects += corrects\n",
    "        \n",
    "#     print(f'epoch #{epoch} | loss: {loss:.2f}, accuracy : {all_corrects/ all_cnt:.3f}')    \n",
    "    \n",
    "    val_corrects, val_cnt = 0, 0\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        val_loss = outputs[0]\n",
    "        predictions = np.array(outputs[1].argmax(-1).to('cpu'))\n",
    "        labels = np.array(batch['labels'].to('cpu'))\n",
    "        corrects = np.sum(np.all(np.equal(predictions, labels), axis=1))\n",
    "        val_cnt += len(batch['labels'])\n",
    "        val_corrects += corrects\n",
    "    val_accuracy = val_corrects/ val_cnt\n",
    "    print(f'epoch #{epoch} | tr_loss:{loss:.2f} tr_acc:{all_corrects/all_cnt} val_loss: {val_loss:.2f}, val_accuracy: {val_accuracy:.3f}')    \n",
    "    \n",
    "    \n",
    "    if  val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        no_imp = 0\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_epoch = epoch\n",
    "    else:\n",
    "        no_imp += 1\n",
    "    if no_imp >= patience:\n",
    "        print(f'terminating... using {best_epoch}')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f1a4a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #12 | loss: 0.12, accuracy : 0.705\n"
     ]
    }
   ],
   "source": [
    "all_corrects, all_cnt = 0, 0\n",
    "best_model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(**batch)\n",
    "    loss = outputs[0]\n",
    "    predictions = np.array(outputs[1].argmax(-1).to('cpu'))\n",
    "    labels = np.array(batch['labels'].to('cpu'))\n",
    "    corrects = np.sum(np.all(np.equal(predictions, labels), axis=1))\n",
    "\n",
    "    all_cnt += len(batch['labels'])\n",
    "    all_corrects += corrects\n",
    "    \n",
    "print(f'epoch #{epoch} | loss: {loss:.2f}, accuracy : {all_corrects/ all_cnt:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb4b9639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "506c12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np\n",
    "# def compute_metrics(p):\n",
    "#     target_max_length = 256\n",
    "#     predictions, labels = p.predictions, p.label_ids\n",
    "    \n",
    "#     predictions = np.pad(predictions, ((0, 0), (0, target_max_length - predictions.shape[1])), mode=\"constant\")\n",
    "#     predictions = np.delete(predictions, 0, axis=1)\n",
    "#     predictions = np.insert(predictions, target_max_length - 1, 0, axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "#     labels = np.array(labels)\n",
    "#     labels = np.pad(labels, ((0, 0), (0, target_max_length - labels.shape[1])), mode=\"constant\")\n",
    "#     labels = np.delete(labels, 0, axis=1)\n",
    "#     labels = np.insert(labels, target_max_length - 1, 0, axis=1)\n",
    "    \n",
    "\n",
    "#     correct_counter = np.sum(np.all(np.equal(labels, predictions), axis=1))\n",
    "#     return {'acc': int(correct_counter)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bb601ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_directory,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=ws,\n",
    "    weight_decay=wd,\n",
    "    logging_dir=model_directory,\n",
    "    logging_steps=100,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=lr,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=1,\n",
    "    eval_accumulation_steps=1,  # set this lower, if testing or validation crashes\n",
    "    disable_tqdm=False,\n",
    "    predict_with_generate=True,  # never set this to false.\n",
    "    seed=42,  # default value\n",
    ")\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=best_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=[torch.optim.Adam(params=model.parameters(), lr=lr), None],\n",
    "    tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1efc8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np\n",
    "# def compute_metrics(p):\n",
    "#     target_max_length = 256\n",
    "#     predictions, labels = p.predictions, p.label_ids\n",
    "    \n",
    "#     predictions = np.pad(predictions, ((0, 0), (0, target_max_length - predictions.shape[1])), mode=\"constant\")\n",
    "#     predictions = np.delete(predictions, 0, axis=1)\n",
    "#     predictions = np.insert(predictions, target_max_length - 1, 0, axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "#     labels = np.array(labels)\n",
    "#     labels = np.pad(labels, ((0, 0), (0, target_max_length - labels.shape[1])), mode=\"constant\")\n",
    "#     labels = np.delete(labels, 0, axis=1)\n",
    "#     labels = np.insert(labels, target_max_length - 1, 0, axis=1)\n",
    "    \n",
    "\n",
    "#     correct_counter = np.sum(np.all(np.equal(labels, predictions), axis=1))\n",
    "#     return {'acc': int(correct_counter)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafc1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ee3518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import EarlyStoppingCallback\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     optimizers=[torch.optim.Adam(params=model.parameters(), lr=lr), None],\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3512e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start_training = datetime.now()\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# end_training = datetime.now()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ff65a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuned_model_dir = f'{model_directory}/best'\n",
    "# tuned_model_dir='/scratch/arminz/tmp/finetuned'\n",
    "trainer.save_model(tuned_model_dir)\n",
    "\n",
    "\n",
    "end_all = datetime.now()\n",
    "# import csv\n",
    "# with open('tuner_runtime.csv', 'a') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     writer.writerow([name, repo, len(train_dataset), len(val_dataset), base_model, start_all, start_training, end_training, end_all])\n",
    "\n",
    "# In[78]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "999967a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local:\n",
    "    from numba import cuda\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cd44b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armin/TFix/env/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:185: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "17it [00:10,  1.82it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:  23:08:41\n",
      "['no-invalid-this', 'no-throw-literal', 'no-new-wrappers', 'guard-for-in', 'no-new-object', 'comma-style', 'prefer-spread', 'no-caller', 'no-extra-bind', 'no-array-constructor', 'prefer-rest-params', 'generator-star-spacing', 'no-this-before-super', 'no-extend-native', 'no-undef', 'no-useless-escape', 'no-dupe-keys', 'no-console', 'no-constant-condition', 'no-duplicate-case', 'no-empty', 'no-extra-semi', 'no-redeclare', 'no-cond-assign', 'no-extra-boolean-cast', 'no-fallthrough', 'no-unreachable', 'valid-typeof', 'no-unsafe-finally', 'no-unused-vars', 'no-debugger', 'no-unsafe-negation', 'no-case-declarations', 'no-self-assign', 'no-process-exit', 'no-inner-declarations', 'for-direction', 'no-compare-neg-zero', 'no-sparse-arrays', 'no-func-assign', 'no-const-assign', 'no-global-assign', 'use-isnan', 'no-unused-labels', 'require-yield', 'getter-return', 'no-dupe-class-members', 'no-ex-assign', 'constructor-super', 'no-new-symbol', 'no-empty-pattern', 'no-class-assign']\n",
      "splitting by : repo-based-included\n",
      "train size: 136\n",
      "val size: 44\n",
      "test size: 51\n",
      "Loaded tokenizer from directory ./storage//tmp/finetuned/curr_118_kibana_1/best\n",
      "Loaded model from directory ./storage//tmp/finetuned/curr_118_kibana_1/best\n",
      "cuda:0\n",
      "Testing has started\n",
      "Number of testing samples:  51\n",
      "score average: 0.49019607843137253 samples_count: 51\n",
      "result : ./storage//testing/10/per-repo/t5-small_test_kibana_10-02-2022_23-08-41\n",
      "end time:  23:09:03\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "result = os.system(f'python hf_transformers/tfix_testing.py --load-model {tuned_model_dir} -bs 16 --model-name t5-small -d repo-based-included -r {repo}')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a28a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(tuned_model_dir)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebecc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460a2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53aed71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7504660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c597ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1ca15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330dafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53729a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942a5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c308b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e1495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
